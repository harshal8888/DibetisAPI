# -*- coding: utf-8 -*-
"""Copy of Diabetes-Predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YOyEA2M_fQtvdwtUOvCWQhk-n7-8YcId

### Importing Modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)

"""### Importing data as a DataFrame"""

#Data Source: https://data.world/abelvikas/diabetes-type-dataset
df = pd.read_csv('Diabetestype.csv')

"""### Get info about the data"""

df.head()

"""## Feature Information

* Age: This is the age of the patients
* BS Fast: This is the blood sugar in fasting (Before Meals), mmol/L
* BS pp: This is the blood sugar in 90 mins after meals mmol/L
* Plasma R: Plasma glucose test randomly taken at any time, mmol/L
* Plasma F: Plasma glucose test fasting usually taken in the morning because it should be taken after at least 8 hours, mmol/L
*  HbA1c: No information provided, mmol/L
"""

df.info() #The data has been cleaned, no missing values, represented with their correct datatype

df.describe()

"""### Data Cleaning"""

df

#Dropping the class feature
df.drop("Class",1,inplace=True)

df

"""# Exploratory Data Analysis"""

plt.figure(figsize=(12,8))
sns.countplot(df.Type)

"""* From the data collected, a lot of Patients are normal, and not diabetic and patients with Type1 diabetic are lesser compared to type2, which makes sense, type1 diabetics are not so common"""

plt.figure(figsize=(30,8))
sns.countplot(df.Type,hue=df.Age)

plt.figure(figsize=(20,8))
sns.boxplot(x='Age',y='BS Fast',data=df)
plt.title('Box Plot of Age with Blood Sugar Level')

plt.figure(figsize=(20,8))
sns.scatterplot(x='Age',y='BS Fast',data=df,hue='Type')
plt.title('Box Plot of Age with Blood Sugar Level')

plt.figure(figsize=(18,15))
sns.heatmap(df.corr(), annot = True, cmap = 'RdYlGn')

"""### Building the ML model"""

#import the libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV

X = df.drop('Type',1)
y = df.iloc[:,-1]

def compute_score(clf, X, y, scoring='accuracy'):
    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)
    return np.mean(xval) #Cross validation to check for biasness and variance

#Testing different base models
logreg = LogisticRegression()
logreg_cv = LogisticRegressionCV()
rf = RandomForestClassifier()
gboost = GradientBoostingClassifier()
models = [logreg, logreg_cv, rf, gboost]

for model in models:
    print('Cross-validation of : {0}'.format(model.__class__))
    score = compute_score(clf=model, X=X, y=y, scoring='accuracy')
    print('CV score = {0}'.format(score))
    print('****')

#Lets try train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

#Using the random forest algorithm
model = rf.fit(X_train,y_train)

y_pred = model.predict(X_test)

#Check the prediction precision and accuracy
from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

#Saving the model with pickle
import pickle

# save the model to disk
model_name  = 'model.pkl'
pickle.dump(model, open(model_name, 'wb'))

print("[INFO]: Finished saving model...")

